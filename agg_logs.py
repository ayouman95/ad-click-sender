#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èšåˆæ—¥å¿—å¹¶å†™å…¥ Elasticsearch ç´¢å¼• click_records_data
æ”¯æŒæ— è®¤è¯ï¼ˆæµ‹è¯•é›†ç¾¤ï¼‰å’Œæœ‰è®¤è¯ï¼ˆç”Ÿäº§ï¼‰ä¸¤ç§æ¨¡å¼
"""

import os
import json
from datetime import datetime, timedelta
from collections import defaultdict
import glob
from pathlib import Path
from elasticsearch import Elasticsearch, helpers

# ==================== é…ç½® ====================
LOG_DIR = "./logs"
LOG_PREFIX = "click.log."
DEBUG_LOG = "agg_debug.log"

# Elasticsearch é…ç½®
ES_HOST = os.getenv("ES_HOST")
ES_USER = os.getenv("ES_NAME")
ES_PASS = os.getenv("ES_PASS")
ES_INDEX = "click_records_data"  # æµ‹è¯•ç´¢å¼•

if not ES_HOST:
    raise EnvironmentError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ï¼šES_HOST")

# å›ºå®šå­—æ®µ
CHANNEL_ID_NUM = 999
CHANNEL = "seed"

def log(msg):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {msg}")
    with open(DEBUG_LOG, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] {msg}\n")

def parse_datetime_from_filename(filename: str):
    basename = Path(filename).name
    if not basename.startswith(LOG_PREFIX):
        return None
    dt_str = basename[len(LOG_PREFIX):]
    if not (len(dt_str) == 12 and dt_str.isdigit()):
        return None
    try:
        year, month, day, hour, minute = (
            int(dt_str[:4]),
            int(dt_str[4:6]),
            int(dt_str[6:8]),
            int(dt_str[8:10]),
            int(dt_str[10:12]),
        )
        return datetime(year, month, day, hour, minute)
    except Exception:
        return None

def find_log_files(log_dir, prefix):
    now = datetime.now() - timedelta(minutes=1)
    last_now = now - timedelta(minutes=1)
    pattern = os.path.join(log_dir, f"{prefix}*")
    matched_files = []
    for file in glob.glob(pattern):
        dt = parse_datetime_from_filename(file)
        if dt and last_now < dt <= now:
            matched_files.append(file)
    return sorted(matched_files)

def extract_time(timestamp_str: str) -> str:
    """æˆªæ–­åˆ°åˆ†é’Ÿï¼Œæ ¼å¼: yyyy-MM-dd HH:mm:ss"""
    return timestamp_str.split(".")[0][:-2] + "00"

def create_es_client():
    """åˆ›å»º Elasticsearch å®¢æˆ·ç«¯ï¼Œæ ¹æ®ç¯å¢ƒå˜é‡å†³å®šæ˜¯å¦ä½¿ç”¨è®¤è¯"""
    try:
        # åˆ¤æ–­æ˜¯å¦æä¾›ç”¨æˆ·åå¯†ç 
        if ES_USER is not None and ES_PASS is not None:
            auth = (ES_USER, ES_PASS)
            log("ğŸ” ä½¿ç”¨è´¦å·å¯†ç è¿æ¥ ES")
            es = Elasticsearch(
                hosts=[ES_HOST],
                basic_auth=auth,
                request_timeout=30,
                verify_certs=True,
                max_retries=3,
                retry_on_timeout=True,
            )
        else:
            log("ğŸ”“ è¿æ¥æ— è®¤è¯çš„æµ‹è¯• ES é›†ç¾¤")
            es = Elasticsearch(
                hosts=[ES_HOST],
                request_timeout=30,
                verify_certs=True,
                max_retries=3,
                retry_on_timeout=True,
            )

        if not es.ping():
            raise ConnectionError("âŒ æ— æ³•è¿æ¥åˆ° Elasticsearch")
        log("âœ… æˆåŠŸè¿æ¥åˆ° Elasticsearch")
        return es
    except Exception as e:
        log(f"âŒ è¿æ¥ ES å¤±è´¥: {e}")
        raise

def process_log_file(filepath: str, aggregator):
    open_func = open
    mode = "r"
    if filepath.endswith(".gz"):
        import gzip
        open_func = gzip.open
        mode = "rt"

    try:
        with open_func(filepath, mode, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue

                if data.get("statusCode") != 200 and data.get("statusCode") != 301:
                    continue

                touch_type = data.get("touchType", "").lower()
                if touch_type not in ("click", "impression"):
                    continue

                raw_time = data.get("time") or data.get("sendTime")
                if not raw_time:
                    continue
                time_key = extract_time(raw_time)

                offer_id_raw = data.get("offerId", "N/A")
                try:
                    offer_id_int = int(offer_id_raw)
                except (TypeError, ValueError):
                    offer_id_int = 0

                key = (
                    time_key,
                    offer_id_int,
                    data.get("channelId", "N/A"),
                    data.get("siteId", "N/A"),
                    data.get("os", "N/A"),
                    data.get("advertiser", "N/A"),
                    data.get("om", "N/A"),
                    data.get("am", "N/A"),
                    data.get("appId", "N/A"),
                    data.get("pid", "N/A"),
                    data.get("geo", "N/A"),
                )

                if key not in aggregator:
                    aggregator[key] = {"clickCount": 0, "impressionCount": 0}

                if touch_type == "click":
                    aggregator[key]["clickCount"] += 1
                elif touch_type == "impression":
                    aggregator[key]["impressionCount"] += 1
    except Exception as e:
        log(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {filepath}: {e}")

def send_to_es(es, index_name, data_list):
    """æ‰¹é‡å†™å…¥ Elasticsearch"""
    if not data_list:
        log("ğŸŸ¡ æ— æ•°æ®å¯å†™å…¥ ES")
        return 0, 0

    actions = [
        {
            "_index": index_name,
            "_source": record,
        }
        for record in data_list
    ]

    try:
        success, failed = helpers.bulk(
            es, actions, raise_on_error=False, ignore_status=[400, 409]
        )
        log(f"âœ… æˆåŠŸå†™å…¥ ES: {success} æ¡")
        if failed:
            log(f"âš ï¸  å†™å…¥å¤±è´¥: {len(failed)} æ¡")
        return success, failed
    except Exception as e:
        log(f"âŒ æ‰¹é‡å†™å…¥ ES å¼‚å¸¸: {e}")
        return 0, len(data_list)

def main():
    log("å¯åŠ¨æ—¥å¿—èšåˆä¸ ES å†™å…¥ä»»åŠ¡")

    if not os.path.exists(LOG_DIR):
        log(f"âŒ ç›®å½•ä¸å­˜åœ¨: {LOG_DIR}")
        return

    log_files = find_log_files(LOG_DIR, LOG_PREFIX)
    if not log_files:
        log("âŒ æ— åŒ¹é…æ—¥å¿—æ–‡ä»¶")
        return

    log(f"âœ… å‘ç° {len(log_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")

    aggregator = {}
    for file_path in log_files:
        log(f"å¤„ç†æ–‡ä»¶: {file_path}")
        process_log_file(file_path, aggregator)

    output_records = []
    for key, counts in sorted(aggregator.items()):
        time_val, offer_id_int, channel_id, site_id, os_val, advertiser, om, am, app_id, pid, geo = key
        # time_val(yyyy-MM-dd HH:mm:ssæ ¼å¼) è½¬æˆæ—¶é—´æˆ³
        dt = int(datetime.strptime(time_val, "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        record = {
            "time": dt,
            "offerId": offer_id_int,
            "offerIdStr": str(offer_id_int),
            "channelId": channel_id,
            "channelIdNum": CHANNEL_ID_NUM,
            "siteId": site_id,
            "os": os_val,
            "advertiser": advertiser,
            "om": om,
            "am": am,
            "appId": app_id,
            "pid": pid,
            "geo": geo,
            "channel": CHANNEL,
            "clickCount": counts["clickCount"],
            "impressionCount": counts["impressionCount"],
        }
        log(f"å†™å…¥ ES: {record}")
        output_records.append(record)

    total = len(output_records)
    log(f"âœ… èšåˆå®Œæˆï¼Œå‡†å¤‡å†™å…¥ ES: {total} æ¡è®°å½•")

    # å†™å…¥ ES
    try:
        es = create_es_client()
        success, failed = send_to_es(es, ES_INDEX, output_records)
        log(f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼å†™å…¥æˆåŠŸ: {success}, å¤±è´¥: {failed}")
    except Exception as e:
        log(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()